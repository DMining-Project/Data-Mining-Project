{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder\n",
    "from pandas_profiling import ProfileReport \n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from math import ceil\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifierCV, LassoCV, RidgeCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('donors.csv', header=0, index_col=[0,'CONTROLN'], low_memory=False, \n",
    "                 dtype={'TCODE':'str','HPHONE_D':'str','RFA_2F':'str','WEALTH1':'str','WEALTH2':'str','INCOME':'str'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for duplicates:\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check df data types:\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace \"\" by nans:\n",
    "df.replace(\"\", np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dealing with the white spaces where they aren´t supposed to be according to the MetaData:\n",
    "lista = []\n",
    "for column in df.columns:\n",
    "    if (df[column].isin([' ']).sum()>0):\n",
    "        lista.append(column)\n",
    "# Decide which ones to replace by NaN or not \n",
    "for feature in lista:\n",
    "    print(df[feature].value_counts())        \n",
    "\n",
    "#variables in which i´ll replace the ' ' by Nan:\n",
    "to_be_replaced=['OSOURCE','PVASTATE','DOMAIN','HOMEOWNR','GENDER','DATASRCE','RFA_3','RFA_4','RFA_5','RFA_6','RFA_7','RFA_8',\n",
    "                'RFA_9','RFA_10','RFA_11','RFA_12','RFA_13','RFA_14','RFA_15','RFA_16','RFA_17','RFA_18','RFA_19','RFA_20',\n",
    "               'RFA_21','RFA_22','RFA_23','RFA_24']\n",
    "\n",
    "for feature in to_be_replaced:\n",
    "    df[feature].replace(\" \", np.nan, inplace=True)\n",
    "    \n",
    "# CHILD12 e assim considerei o ' 'como ausencia de filhos e nao como missing values\n",
    "\n",
    "# In the variables 'COLLECT1'-'PLATES' the white space corresponds to the category 'N':\n",
    "for feature in df.columns[52:70]:\n",
    "    df[feature].replace(\" \", 'N', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted count of missing values:\n",
    "df.isnull().sum()[df.isnull().sum()>0].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking descriptive statistics:\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"GENDER\"] = df[\"GENDER\"].apply(lambda x: \"U\" if (x != \"F\" and x != \"M\") else x).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"ODATEDW\"] = df[\"ODATEDW\"].apply(lambda x: x[:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DOB'] = df['DOB'].apply(lambda x: int(str(x)[:4]) if (str(x)!='nan') else None)\n",
    "df['DOB'] = 2020 - df['DOB']\n",
    "df.rename(columns={'DOB':'Age'},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the distribution of missing values(greater than 10%)\n",
    "ax = sns.histplot(df.isnull().mean()*100, bins='auto')\n",
    "ax.set_title('Distribution of Missing values (%) ')\n",
    "ax.set(xlabel='% of missing values', ylabel='Count of features',xlim=(10,100),ylim=(0,30))\n",
    "# Colloring red the bars that represent the count of features with % of missing values >= 50% :\n",
    "for i in range(5,10):\n",
    "    ax.patches[i].set_color('r')\n",
    "    ax.patches[i].set_linestyle(':')\n",
    "plt.show(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing the features that have missing values\n",
    "\n",
    "We are removing all the columns that have more than 50% of Missing Values\n",
    "\n",
    "### Nota: Ver as ADATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cols = df.columns[df.isnull().mean() > 0.5]\n",
    "\n",
    "display(pd.DataFrame(df[cols].isnull().mean()*100, columns=['% Missing Values']).sort_values(by='% Missing Values',ascending=False))\n",
    "\n",
    "df = df.drop(cols, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metric and Non-Metric Variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_metric_feat(df):\n",
    "    metric_features = df.select_dtypes(include=np.number).columns\n",
    "    non_metric_features = df.select_dtypes(exclude=np.number).columns\n",
    "    return metric_features.tolist(),non_metric_features.tolist()\n",
    "\n",
    "metric_features = update_metric_feat(df)[0]\n",
    "non_metric_features = update_metric_feat(df)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Box Plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# All Numeric Variables' Box Plots in one figure\n",
    "sns.set()\n",
    "\n",
    "# Prepare figure. Create individual axes where each box plot will be placed\n",
    "fig, axes = plt.subplots(52, ceil(len(metric_features) / 52), figsize=(50, 50), squeeze=True)\n",
    "\n",
    "# Plot data\n",
    "# Iterate across axes objects and associate each box plot:\n",
    "for ax, feat in zip(axes.flatten(), metric_features): \n",
    "    sns.boxplot(x=df[feat], ax=ax)\n",
    "#     ax.set_title(feat, y=-0.21)\n",
    "\n",
    "# Adjusting my subplots:\n",
    "fig.tight_layout()\n",
    "\n",
    "# Layout\n",
    "# Add a centered title to the figure:\n",
    "title = \"Numeric Variables' Box Plots\"\n",
    "plt.suptitle(title)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "- 1)Impute the missing values with the median for the metric features and mode for the non metric, because those measures are robust to outliers\n",
    "- 2)Drop outliers using the Local Outlier Factor method or/and its intersection with the methods IQR and z-score\n",
    "- 3)Now that we don´t have outliers anymore we can use a better method for the missing data imputation\n",
    "- 4)After the feature selection we can go back and only drop the outliers on the variables that we kept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an auxiliary df ( so that later I can use another method for the data imputation), and Filling the Categorical Variables with the mode and the Numeric Variables with the Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_df = df.copy()\n",
    "\n",
    "for column in aux_df[metric_features]:\n",
    "    aux_df[column] = aux_df[column].fillna(aux_df[column].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I can drop the outliers on our original df, based on our aux_df without missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dealing with outliers on the metric features:\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "# we are only finding out outliers among the metric features:\n",
    "aux_df = aux_df[metric_features]\n",
    "# identifying outliers in our dataset\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(aux_df.values)\n",
    "\n",
    "# select all rows that are not outliers\n",
    "mask = yhat != -1\n",
    "\n",
    "# variable that contains the original number of observations:\n",
    "numero_obs_original = df.shape[0]\n",
    "df = df.drop(index=aux_df.iloc[~mask, :].index)\n",
    "\n",
    "\n",
    "print('% eliminated=',(1-len(df)/numero_obs_original)*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our df is free of outliers so we can make a better imputation of the missing values in the metric features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Later when we have done all of our feature selection, we can go back here and apply the following methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score_outlier_detection(df,column):\n",
    "    mean = df[column].mean()\n",
    "    std = df[column].std()\n",
    "    threshold = 3\n",
    "    z_s = (df[column]-mean)/std\n",
    "    return z_s.apply(lambda z: z>threshold) # true if outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function that removes the outliers from the metric features, using the IQR method:\n",
    "def iqr_outlier_detection(df, column):\n",
    "    q1 = df[column].quantile(0.25)\n",
    "    q3 = df[column].quantile(0.75)\n",
    "    iqr = (q3 - q1) \n",
    "    lower_lim = q1 - 5 * iqr\n",
    "    upper_lim = q3 + 5 * iqr\n",
    "    return df[column].apply(lambda x: (x < lower_lim or x > upper_lim))   # true if outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aux_df = df.copy()\n",
    "\n",
    "# aux_df['outlier_z_score?']=False\n",
    "# aux_df['outlier_iqr?']=False\n",
    "\n",
    "# for feature in metric_features:\n",
    "#     aux_df['outlier_z_score?'] = aux_df['outlier_z_score?']|z_score_outlier_detection(aux_df,feature)\n",
    "#     aux_df['outlier_iqr?'] = aux_df['outlier_iqr?']|iqr_outlier_detection(aux_df,feature)\n",
    "\n",
    "# teste =aux_df['outlier_z_score?'] & aux_df['outlier_iqr?']\n",
    "# print('% to be eliminated=',(teste.sum()/95412)*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill missing values (Data imputation)\n",
    "\n",
    "Filling the Categorical Variables with the mode and the Numeric Variables with the KNN imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non Metric features:\n",
    "for column in df[non_metric_features]:\n",
    "    df[column] = df[column].fillna(df[column].mode()[0])\n",
    "\n",
    "# Metric features:\n",
    "# Creating new df copy to explore neighbordhood imputation\n",
    "df_neighbors = df.copy()\n",
    "\n",
    "# Seeing rows with NaNs\n",
    "nans_index = df_neighbors.isna().any(axis=1)\n",
    "df_neighbors[nans_index]\n",
    "\n",
    "# KNNImputer - only works for numerical varaibles\n",
    "imputer = KNNImputer(n_neighbors=5, weights=\"uniform\")\n",
    "df_neighbors[metric_features] = imputer.fit_transform(df_neighbors[metric_features])\n",
    "\n",
    "# See rows with NaNs imputed\n",
    "df_neighbors.loc[nans_index, metric_features]\n",
    "\n",
    "# let's keep the df_neighbors dataframe\n",
    "df = df_neighbors.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low Variance\n",
    "\n",
    "Removing variables with low Variance\n",
    "\n",
    "Done on a scaled data using MinMax Scaler\n",
    "\n",
    "If the variance is too low, it means that it does not change much and hence it can be ignored\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -> Function that plots Histograms of a set of variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histograms(df,features, bins='auto',figsize=(15, 8)):\n",
    "    # Put this Variables' Histograms in one figure\n",
    "    sns.set()\n",
    "\n",
    "    # Prepare figure. Create individual axes where each histogram will be placed\n",
    "    fig, axes = plt.subplots(ceil(len(features) / 4),4, figsize=figsize)\n",
    "\n",
    "    # Plot data\n",
    "    # Iterate across axes objects and associate each histogram:\n",
    "    for ax, feat in zip(axes.flatten(), features):\n",
    "        ax.hist(df[feat], bins=bins)\n",
    "        ax.set_title(feat, y=-0.17)\n",
    "\n",
    "    # Layout\n",
    "    # Add a centered title to the figure:\n",
    "    title = \"Variables' Histograms\"\n",
    "    plt.suptitle(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scalling our data:\n",
    "scaler = MinMaxScaler()\n",
    "# transform data:\n",
    "scaled = scaler.fit_transform(df[metric_features])\n",
    "scaled = pd.DataFrame(scaled,columns = metric_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_variance_features = []\n",
    "for feature in metric_features:\n",
    "    if (scaled[feature].var())<0.0009:\n",
    "        low_variance_features.append(feature)\n",
    "\n",
    "print('The following are redundant variables because of their low variance:\\n',low_variance_features)\n",
    "\n",
    "#Plotting an histogram of those variables:\n",
    "histograms(df,low_variance_features, bins='auto',figsize=(35, 50))\n",
    "\n",
    "# Dropping (note: I wont drop the boolean variable HPHONE_D because it only takes 0 and 1 values so it will easily have a std \n",
    "# value above my threshold, but I won´t consider it has being redundant):\n",
    "df.drop(columns=low_variance_features, inplace=True)\n",
    "\n",
    "#updating metric features list:\n",
    "metric_features = update_metric_feat(df)[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pd.DataFrame(scaled[metric_features].var(), columns=['Variance']).sort_values(by='Variance'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation\n",
    "Remove the features that have high correlation values (threshold of 70%) -> To avoid multicolinearity\n",
    "\n",
    "For each pair of correlated features, our method identifies one of the features for removal (because we only need to remove one of them)\n",
    "\n",
    "Make visualization with the high correlations (ex:heatmap that shows all the features that have at least one correlation above the threshold)\n",
    "\n",
    "Make list of correlated features that will be removed, or show the highly correlated pairs of features in a dataframe\n",
    "\n",
    "Features with high correlation are more linearly dependent and hence have almost the same effect on the dependent variable. So, when two features have high correlation, we can drop one of the two features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -> Function that plots an heatmap of the correlation matrix passed as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_heatmap(corr):    \n",
    "    # Prepare figure\n",
    "    fig = plt.figure(figsize=(50, 30))\n",
    "    # Build annotation matrix (values above |0.7| will appear annotated in the plot)\n",
    "    mask_annot = np.absolute(corr.values) >= 0.7\n",
    "    annot = np.where(mask_annot, corr.values, np.full(corr.shape,\"\"))\n",
    "    # Plot heatmap of the correlation matrix\n",
    "    sns.heatmap(data=corr, annot=annot, cmap=sns.diverging_palette(220, 10, as_cmap=True), \n",
    "                fmt='s', vmin=-1, vmax=1, center=0, square=True, linewidths=.5, annot_kws={\"size\": 10})\n",
    "    # Layout\n",
    "    fig.subplots_adjust(top=0.95)\n",
    "    fig.suptitle(\"Correlation Matrix\", fontsize=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -> Handling with highly correlated Features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating correlation matrix with the absolute values of the corr:\n",
    "corr = df.corr().abs()\n",
    "\n",
    "# Selecting upper triangle of the correlation matrix:\n",
    "upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(np.bool))\n",
    "\n",
    "# Finding the index of the feature's columns with correlation greater than 0.70 (>=very strong correlation)\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.70)]\n",
    "\n",
    "print('The features to be removed because of their high correlation are:\\n',to_drop)\n",
    "\n",
    "# Saving the pairs of correlated features so I can plot an heatmap of their correlations:\n",
    "\n",
    "# I´ll save both the dropped feature and the one that was correlated with it:\n",
    "records = pd.DataFrame(columns = ['dropped_feat', 'corr_feat'])\n",
    "for column in to_drop:\n",
    "    corr_features = list(upper.index[upper[column].abs() > 0.7])\n",
    "    drop_features = [column for _ in range(len(corr_features))]\n",
    "    # Record the information (need a temp df for now)\n",
    "    aux_df = pd.DataFrame.from_dict({'dropped_feat': drop_features,'corr_feat': corr_features})\n",
    "\n",
    "    # Add to dataframe\n",
    "    records = records.append(aux_df, ignore_index = True)\n",
    "\n",
    "\n",
    "corr_matrix_plot = corr.loc[list(set(records['corr_feat'])),list(set(records['dropped_feat']))]\n",
    "title = \"Correlations Above Threshold\"\n",
    "# * the features on the x axis are the ones that are going to be removed\n",
    "\n",
    "\n",
    "# Plotting an heatmap:\n",
    "corr_heatmap(np.round(corr_matrix_plot,decimals=2))\n",
    "\n",
    "\n",
    "# Dropping the highly correlated features \n",
    "df.drop(columns = to_drop, inplace=True)\n",
    "\n",
    "# updating metric features list:\n",
    "metric_features = update_metric_feat(df)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some explanations\n",
    "\n",
    "[Handle with variables of central tendency measures]\n",
    "\n",
    "Once there are variables that provide the same information but in different measures, we decide to keep the variables that refer to the median because it's less sensitive to outliers and remove the others ones in these specific cases.\n",
    "\n",
    "One example of the situation described above is the variable HHP1 (Median Person Per Household) and HHP2 (Average Person Per Household).\n",
    "\n",
    "\n",
    "[Handle with redundant variables]\n",
    "\n",
    "Since there are variables that have repeated information, that is, variables that englobe other variables, let's take into consideration, for example, HHN3 (Percent 3 or More Person Households) and HHN4 (Percent 4 or More Person Households). We can soon conclude that HHN4 is mentioned in HHN3, once 'more than 3' englobe the percent of 4 and so on.\n",
    "\n",
    "In these particular cases, we decide to keep the variables that provide the same meaning as the others of the same kind, without losing information, and discard the ones that are already indirectly specified in the ones that we chose to keep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale/Standardize our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_standard = df.copy()\n",
    "scaler = StandardScaler()\n",
    "scaled_feat = scaler.fit_transform(df_standard[metric_features])\n",
    "scaled_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# See what the fit method is doing :\n",
    "print(\"Parameters fitted:\\n\", scaler.mean_, \"\\n\", scaler.var_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_standard[metric_features] = scaled_feat\n",
    "df_standard.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking mean and variance of standardized variables\n",
    "df_standard[metric_features].describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_standard.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply kmeans for feature selection (using it to find the clusters which then are my target variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the number of clusters:\n",
    "range_clusters = range(1, 11)\n",
    "inertia = []\n",
    "for n_clus in range_clusters:  # iterate over desired ncluster range\n",
    "    kmclust = KMeans(n_clusters=n_clus, init='k-means++', n_init=15, random_state=1)\n",
    "    kmclust.fit(df[metric_features])\n",
    "    inertia.append(kmclust.inertia_)  # save the inertia of the given cluster solution\n",
    "    \n",
    "# The inertia plot\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.plot(inertia)\n",
    "plt.ylabel(\"Inertia: SSw\")\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.title(\"Inertia plot over clusters\", size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final cluster solution\n",
    "number_clusters = 3\n",
    "kmclust = KMeans(n_clusters=number_clusters, init='k-means++', n_init=15, random_state=0)\n",
    "km_labels = kmclust.fit_predict(df[metric_features])\n",
    "df_concat = df.copy() \n",
    "df_concat['labels']=km_labels\n",
    "df_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso regression\n",
    "# separating the income variable from the independent numeric variables:\n",
    "data = df_concat[metric_features]\n",
    "target = df_concat.iloc[:,-1]\n",
    "\n",
    "reg = LassoCV()\n",
    "reg.fit(X = data, y = target)\n",
    "\n",
    "# The alpha value is associated to the regularization strength. Regularization improves the conditioning of the problem and \n",
    "# reduces the variance of the estimates. Larger values specify stronger regularization:\n",
    "print(\"Best alpha using built-in LassoCV: %f\" % reg.alpha_)\n",
    "\n",
    "# The score return the coefficient of determination R^2 of the prediction:\n",
    "print(\"Best score using built-in LassoCV: %f\" %reg.score(data,target))\n",
    "\n",
    "# coef will have as index the data columns names and the coef_ attribute as value:\n",
    "coef = pd.Series(reg.coef_, index = data.columns)\n",
    "coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")\n",
    "coef.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_importance(coef,name):\n",
    "    imp_coef = coef.sort_values()\n",
    "    plt.figure(figsize=(55,55))\n",
    "    imp_coef.plot(kind = \"barh\")\n",
    "    plt.title(\"Feature importance using \" + name + \" Model\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "plot_importance(coef, \"Lasso\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ridge classifier\n",
    "# ridge = RidgeClassifierCV().fit(X = data,y = target)\n",
    "# coef_ridge = pd.Series(ridge.coef_[0], index = data.columns)\n",
    "# plot_importance(coef_ridge,'RidgeClassifier')\n",
    "\n",
    "# ridge.score(data,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the variables according to the Lasso Regression:\n",
    "to_drop = np.array(metric_features)[(coef ==0).values]\n",
    "\n",
    "print('The following variables were dropped because according to the Lasso Regression they weren´t important to determine the \\\n",
    "cluster:', to_drop)\n",
    "\n",
    "df.drop(columns=to_drop, inplace=True)\n",
    "\n",
    "# updating metric features list:\n",
    "metric_features = update_metric_feat(df)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca = df.copy()\n",
    "\n",
    "# Use PCA to reduce dimensionality of data\n",
    "pca = PCA()\n",
    "pca_feat = pca.fit_transform(df_pca[metric_features])\n",
    "# pca_feat  \n",
    "\n",
    "var_explained = pca.explained_variance_ratio_.cumsum()\n",
    "\n",
    "pca.explained_variance_ratio_[:10].sum()\n",
    "\n",
    "var_explained = var_explained*100 #percentage\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "plt.bar(range(len(var_explained)), var_explained, label='Cumulative _Sum_of_Explained _Varaince', color = 'b',alpha=0.5)\n",
    "plt.title(\"Around 80% of variance is explained by the first 40 principal components \");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca_feat.shape)\n",
    "print(df_pca[metric_features].shape)\n",
    "pd.DataFrame(pca_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_matrix = df_pca[metric_features].cov()\n",
    "cov_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Composing back the covariance matrix\n",
    "pd.DataFrame(pca.components_.T @ np.diag(pca.explained_variance_) @ pca.components_, \n",
    "             index=cov_matrix.index, \n",
    "             columns=cov_matrix.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining the projected observations on the principal components axes (linear combinations)\n",
    "pd.DataFrame(df_pca[metric_features].values @ pca.components_.T, \n",
    "             index=df_pca.index,\n",
    "             columns=[f\"PC{i}\" for i in range(pca.n_components_)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output PCA table\n",
    "pd.DataFrame(\n",
    "    {\"Eigenvalue\": pca.explained_variance_,\n",
    "     \"Difference\": np.insert(np.diff(pca.explained_variance_), 0, 0),\n",
    "     \"Proportion\": pca.explained_variance_ratio_,\n",
    "     \"Cumulative\": np.cumsum(pca.explained_variance_ratio_)},\n",
    "    index=range(1, pca.n_components_ + 1)\n",
    ").head(41)\n",
    "# rule(pearson), de manter os que têm um eigenvalue>1 ->>>> manter 23 PC\n",
    "# na kaiser´s ruler queremos reter 80% da variancia, olhar para a comulative ->>>> manter 40 PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure and axes\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(55, 25))\n",
    "\n",
    "# draw plots\n",
    "ax1.plot(pca.explained_variance_, marker=\".\", markersize=12)\n",
    "ax2.plot(pca.explained_variance_ratio_, marker=\".\", markersize=12, label=\"Proportion\")\n",
    "ax2.plot(np.cumsum(pca.explained_variance_ratio_), marker=\".\", markersize=12, linestyle=\"--\", label=\"Cumulative\")\n",
    "\n",
    "# customizations\n",
    "ax2.legend()\n",
    "ax1.set_title(\"Scree Plot\", fontsize=14)\n",
    "ax2.set_title(\"Variance Explained\", fontsize=14)\n",
    "ax1.set_ylabel(\"Eigenvalue\")\n",
    "ax2.set_ylabel(\"Proportion\")\n",
    "ax1.set_xlabel(\"Components\")\n",
    "ax2.set_xlabel(\"Components\")\n",
    "ax1.set_xticks(range(0, pca.n_components_, 2))\n",
    "ax1.set_xticklabels(range(1, pca.n_components_ + 1, 2))\n",
    "ax2.set_xticks(range(0, pca.n_components_, 2))\n",
    "ax2.set_xticklabels(range(1, pca.n_components_ + 1, 2))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ESCOLHER OS PRINCIPAL COMPONENTS A RETER E FAZER OS CODIGOS SEGUINTES:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Perform PCA again with the number of principal components you want to retain\n",
    "# pca = PCA(n_components=23)\n",
    "# pca_feat = pca.fit_transform(df_pca[metric_features])\n",
    "# pca_feat_names = [f\"PC{i}\" for i in range(pca.n_components_)]\n",
    "# pca_df = pd.DataFrame(pca_feat, index=df_pca.index, columns=pca_feat_names)  # remember index=df_pca.index\n",
    "# pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reassigning df to contain pca variables\n",
    "# df_pca = pd.concat([df_pca, pca_df], axis=1)\n",
    "# df_pca.head()\n",
    "\n",
    "# # we should use the components for our analysis instead of the numeric variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INTERPRETATION OF THE PRINCIPAL COMPONENTS AND LOADINGS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -> Pieplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "\n",
    "# Prepare figure. Create individual axes where each histogram will be placed\n",
    "fig, axes = plt.subplots(23, 2, figsize=(20, 80))\n",
    "\n",
    "# Plot data\n",
    "# Iterate across axes objects and associate each histogram:\n",
    "for ax, feat in zip(axes.flatten(), non_metric_features): # Notice the zip() function and flatten() method\n",
    "    ax.pie(df[feat].value_counts().values,labels=df[feat].value_counts().index,autopct='%1.1f%%', shadow=True, radius=2)\n",
    "    ax.set_title(feat)\n",
    "fig.tight_layout()\n",
    "# Layout\n",
    "# Add a centered title to the figure:\n",
    "title = \" Non metric features Pieplots\"\n",
    "plt.suptitle(title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -> Single Value Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of unique values for each column using numpy\n",
    "counts = list(df.nunique())\n",
    "names = list(df.nunique().index)\n",
    "\n",
    "def create_dict(keys, values): \n",
    "    return dict(zip(keys, values + [None] * (len(keys) - len(values))))\n",
    "columns = create_dict(names, counts)\n",
    "\n",
    "# record columns to delete\n",
    "to_del = dict((k, v) for k, v in columns.items() if v == 1) \n",
    "to_del"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -> Remove the variables that for one category have more than 85% of the observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop=[]\n",
    "for feature in non_metric_features:\n",
    "    for proportion in df[feature].value_counts()/len(df):\n",
    "        if proportion > 0.85:\n",
    "            to_drop.append(feature)\n",
    "            break\n",
    "\n",
    "print('The non metric features that have more than 85% in one of their categories/unique values (to be dropped because \\\n",
    "of their redundancy) are the following:\\n',to_drop)\n",
    "\n",
    "df.drop(columns=to_drop, inplace=True)\n",
    "\n",
    "# updating non metric features list:\n",
    "non_metric_features = update_metric_feat(df)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ->Drop the categorical features with high variability , in case none of its categories have more than 15%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop=[]\n",
    "for feature in non_metric_features:\n",
    "    aux =[]\n",
    "    for proportion in df[feature].value_counts()/len(df):\n",
    "        if proportion < 0.15:\n",
    "            aux.append(proportion)\n",
    "    if len(aux)==len(df[feature].value_counts()): # if all categories have less than 15%;\n",
    "        to_drop.append(feature)\n",
    "\n",
    "print('The non metric features that have less than 15% in all of their categories/unique values (to be dropped because \\\n",
    "of their redundancy) are the following:\\n',to_drop)\n",
    "\n",
    "df.drop(columns=to_drop, inplace=True)\n",
    "\n",
    "# updating non metric features list:\n",
    "non_metric_features = update_metric_feat(df)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce Cardinality of the Categorical features!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot with the cardinality of each variable\n",
    "\n",
    "df[non_metric_features].nunique().plot.bar(figsize=(12,6))\n",
    "plt.ylabel('Number of unique categories')\n",
    "plt.xlabel('Variables')\n",
    "plt.title('Cardinality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Modes and feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
